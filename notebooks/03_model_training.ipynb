{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171aff9b",
   "metadata": {},
   "source": [
    "# Model Training - Bengali Hate Speech Detection\n",
    "\n",
    "**Objective:** Build and train models for multi-label hate speech classification\n",
    "\n",
    "**Approach:**\n",
    "1. Create baseline model (Traditional ML)\n",
    "2. Fine-tune Bengali BERT for multi-label classification\n",
    "3. Compare performance and analyze results\n",
    "\n",
    "**Data:** Preprocessed Dataset 1 (16,068 samples, 6 labels)\n",
    "**Target:** Multi-label classification (vulgar, hate, religious, threat, troll, Insult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c45ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Training set: 11253 samples\n",
      "Validation set: 2404 samples\n",
      "Test set: 2411 samples\n",
      "Labels: ['vulgar', 'hate', 'religious', 'threat', 'troll', 'Insult']\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load the preprocessed data splits\n",
    "train_df = pd.read_csv('../data/train_dataset1.csv')\n",
    "val_df = pd.read_csv('../data/val_dataset1.csv')\n",
    "test_df = pd.read_csv('../data/test_dataset1.csv')\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\") \n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "# Define label columns\n",
    "label_columns = ['vulgar', 'hate', 'religious', 'threat', 'troll', 'Insult']\n",
    "print(f\"Labels: {label_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875e5f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for baseline models...\n",
      "Data shapes:\n",
      "X_train: (11253,)\n",
      "y_train: (11253, 6)\n",
      "X_val: (2404,)\n",
      "y_val: (2404, 6)\n",
      "\n",
      "Label distribution in training set:\n",
      "  vulgar: 1751 (15.6%)\n",
      "  hate: 1342 (11.9%)\n",
      "  religious: 1001 (8.9%)\n",
      "  threat: 982 (8.7%)\n",
      "  troll: 1160 (10.3%)\n",
      "  Insult: 1890 (16.8%)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for traditional ML baseline\n",
    "print(\"Preparing data for baseline models...\")\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = train_df['text']\n",
    "y_train = train_df[label_columns]\n",
    "\n",
    "X_val = val_df['text'] \n",
    "y_val = val_df[label_columns]\n",
    "\n",
    "X_test = test_df['text']\n",
    "y_test = test_df[label_columns]\n",
    "\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")\n",
    "\n",
    "# Check label distribution\n",
    "print(\"\\nLabel distribution in training set:\")\n",
    "for label in label_columns:\n",
    "    count = y_train[label].sum()\n",
    "    pct = (count / len(y_train)) * 100\n",
    "    print(f\"  {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4edc404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TF-IDF + SVM baseline model...\n",
      "TF-IDF feature shape: (11253, 10000)\n",
      "Vocabulary size: 10000\n",
      "Training SVM classifier...\n",
      "SVM baseline training completed!\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF baseline model\n",
    "print(\"Training TF-IDF + SVM baseline model...\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,  # Limit features for efficiency\n",
    "    min_df=2,           # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.95,        # Ignore terms that appear in more than 95% of documents\n",
    "    ngram_range=(1, 2), # Use unigrams and bigrams\n",
    "    stop_words=None     # No Bengali stop words removal for now\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "\n",
    "print(f\"TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "\n",
    "# Multi-output SVM classifier\n",
    "svm_classifier = MultiOutputClassifier(\n",
    "    SVC(kernel='linear', C=1.0, random_state=42, probability=True),\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training SVM classifier...\")\n",
    "svm_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions on validation set\n",
    "y_val_pred_svm = svm_classifier.predict(X_val_tfidf)\n",
    "y_val_pred_proba_svm = svm_classifier.predict_proba(X_val_tfidf)\n",
    "\n",
    "print(\"SVM baseline training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341b0068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM baseline model...\n",
      "\n",
      "SVM Baseline Performance:\n",
      "========================================\n",
      "Exact match accuracy: 0.5408\n",
      "Macro-averaged Precision: 0.6700\n",
      "Macro-averaged Recall: 0.2115\n",
      "Macro-averaged F1-score: 0.3061\n",
      "\n",
      "Per-label performance:\n",
      "  vulgar      : P=0.812 R=0.269 F1=0.405 Support=386\n",
      "  hate        : P=0.591 R=0.232 F1=0.333 Support=280\n",
      "  religious   : P=0.805 R=0.404 F1=0.538 Support=225\n",
      "  threat      : P=0.605 R=0.121 F1=0.202 Support=214\n",
      "  troll       : P=0.600 R=0.014 F1=0.027 Support=218\n",
      "  Insult      : P=0.606 R=0.228 F1=0.331 Support=413\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model performance\n",
    "print(\"Evaluating SVM baseline model...\")\n",
    "\n",
    "def evaluate_multilabel_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate multi-label classification performance\"\"\"\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Overall metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Exact match accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Per-label metrics\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro averages\n",
    "    macro_precision = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)[0]\n",
    "    macro_recall = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)[1] \n",
    "    macro_f1 = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)[2]\n",
    "    \n",
    "    print(f\"Macro-averaged Precision: {macro_precision:.4f}\")\n",
    "    print(f\"Macro-averaged Recall: {macro_recall:.4f}\")\n",
    "    print(f\"Macro-averaged F1-score: {macro_f1:.4f}\")\n",
    "    \n",
    "    # Per-label breakdown\n",
    "    print(f\"\\nPer-label performance:\")\n",
    "    for i, label in enumerate(label_columns):\n",
    "        print(f\"  {label:12s}: P={precision[i]:.3f} R={recall[i]:.3f} F1={f1[i]:.3f} Support={support[i]}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_precision': macro_precision,\n",
    "        'macro_recall': macro_recall,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_label_f1': f1\n",
    "    }\n",
    "\n",
    "# Evaluate SVM baseline\n",
    "svm_results = evaluate_multilabel_model(y_val, y_val_pred_svm, \"SVM Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39eb17fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers already installed: 4.52.4\n",
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for BERT (run this once)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"Successfully installed {package}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing {package}: {e}\")\n",
    "\n",
    "# Install transformers if not already installed\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers already installed: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing transformers...\")\n",
    "    install_package(\"transformers\")\n",
    "    install_package(\"datasets\")\n",
    "    install_package(\"accelerate\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Please install PyTorch first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f71cc8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for BERT...\n",
      "Loaded tokenizer: sagorsarker/bangla-bert-base\n",
      "Vocab size: 101975\n",
      "\n",
      "Sample text: এবার পাগলামি বন্ধ করো আর কত।\n",
      "Tokens (8): ['এবার', 'পাগলামি', 'বনধ', 'করে', '##া', 'আর', 'কত', '।']...\n",
      "Created datasets:\n",
      "  Training: 11253 samples\n",
      "  Validation: 2404 samples\n",
      "\n",
      "Sample data shapes:\n",
      "  input_ids: torch.Size([256])\n",
      "  attention_mask: torch.Size([256])\n",
      "  labels: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for BERT training\n",
    "print(\"Preparing data for BERT...\")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load Bengali BERT tokenizer\n",
    "model_name = \"sagorsarker/bangla-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_name}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = train_df['text'].iloc[0]\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(f\"\\nSample text: {sample_text}\")\n",
    "print(f\"Tokens ({len(tokens)}): {tokens[:10]}...\")\n",
    "\n",
    "class BengaliHateSpeechDataset(Dataset):\n",
    "    \"\"\"Custom dataset for Bengali hate speech classification\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Get labels\n",
    "        labels = torch.FloatTensor(self.labels.iloc[idx].values)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BengaliHateSpeechDataset(\n",
    "    X_train.reset_index(drop=True), \n",
    "    y_train.reset_index(drop=True), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = BengaliHateSpeechDataset(\n",
    "    X_val.reset_index(drop=True),\n",
    "    y_val.reset_index(drop=True), \n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "print(f\"Created datasets:\")\n",
    "print(f\"  Training: {len(train_dataset)} samples\")\n",
    "print(f\"  Validation: {len(val_dataset)} samples\")\n",
    "\n",
    "# Test dataset\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\nSample data shapes:\")\n",
    "for key, value in sample.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d954da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: sagorsarker/bangla-bert-base\n",
      "Number of labels: 6\n",
      "Model parameters: 164,401,158\n",
      "Using device: cuda\n",
      "Training arguments configured.\n"
     ]
    }
   ],
   "source": [
    "# Create BERT model for multi-label classification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load pre-trained Bengali BERT\n",
    "num_labels = len(label_columns)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define training arguments (fixed parameter names)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",  # Changed from evaluation_strategy\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e34a523c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom metrics function defined.\n"
     ]
    }
   ],
   "source": [
    "# Define custom metrics for multi-label classification\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for multi-label classification\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid to get probabilities\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    \n",
    "    # Convert to binary predictions (threshold = 0.5)\n",
    "    y_pred = (probs > 0.5).int().numpy()\n",
    "    y_true = labels.astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "    }\n",
    "\n",
    "print(\"Custom metrics function defined.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
